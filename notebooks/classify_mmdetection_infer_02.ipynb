{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from cellpose import models\n",
    "from cellpose.io import imread\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageSequence\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os.path\n",
    "# from livecellx import segment\n",
    "from livecellx import core\n",
    "from livecellx.core import datasets\n",
    "from livecellx.core.datasets import LiveCellImageDataset, SingleImageDataset\n",
    "from skimage import measure\n",
    "from livecellx.core import SingleCellTrajectory, SingleCellStatic\n",
    "# import detectron2\n",
    "# from detectron2.utils.logger import setup_logger\n",
    "\n",
    "# setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate single cells from a mask dataset and write single cell json data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = \"XY1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset_dir_path = Path(\n",
    "#     f\"../datasets/EBSS_Starvation/tif_STAV-A549_VIM_24hours_NoTreat_NA_YL_Ti2e_2022-12-21/{pos}/\"\n",
    "# )\n",
    "\n",
    "# mask_dataset_path = Path(f\"../datasets/EBSS_Starvation/tif_STAV-A549_VIM_24hours_NoTreat_NA_YL_Ti2e_2022-12-21/out/{pos}/seg\")\n",
    "\n",
    "# mask_dataset = LiveCellImageDataset(mask_dataset_path, ext=\"png\")\n",
    "# time2url = sorted(glob.glob(str((Path(dataset_dir_path) / Path(\"*_DIC.tif\")))))\n",
    "# time2url = {i: path for i, path in enumerate(time2url)}\n",
    "# dic_dataset = LiveCellImageDataset(time2url=time2url, ext=\"tif\")\n",
    "\n",
    "# from livecellx.segment.utils import prep_scs_from_mask_dataset\n",
    "# single_cells = prep_scs_from_mask_dataset(mask_dataset, dic_dataset)\n",
    "\n",
    "# scs_write_path = f\"./datasets/test_scs_EBSS_starvation/{pos}/scs.json\"\n",
    "# dataset_dir = Path(f\"./datasets/test_scs_EBSS_starvation/{pos}/datasets\")\n",
    "# SingleCellStatic.write_single_cells_json(single_cells, scs_write_path, dataset_dir=dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v6-combined-clipL=2-CLIP_NUM=4/\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v8-combined-clipLen=2-trainClipNum=3-valClipNum=3\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v8-combined-clipLen=1-trainClipNum=3-valClipNum=3\")\n",
    "# path = r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v6-combined-clipL=2/best_acc_top1_epoch_28.pth\"\n",
    "# model = torch.load(path)\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v9-combined-clipLen=3-trainClipNum=3-valClipNum=3\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v10-st-combined-clipLen=3-trainClipNum=3-valClipNum=3\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v10-drop-div-combined-clipLen=2-trainClipNum=3-valClipNum=3\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v12-st-combined-clipLen=2-trainClipNum=3-valClipNum=3/\")\n",
    "# out_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/test_results/v12-st-combined-clipLen=2-trainClipNum=3\")\n",
    "\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v12-st-combined-clipLen=2-trainClipNum=3-valClipNum=3/\")\n",
    "# out_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/test_results/v12-st-combined-clipLen=2-trainClipNum=3\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v12-st-video-clipLen=2-trainClipNum=3-valClipNum=3/\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-st-combined-random-crop/\")\n",
    "# out_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/test_results/timesformer-default-divst-v13-st-combined-random-crop-epoch=24\")\n",
    "\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-st-combined-random-crop/\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-inclusive-with-mitosis-type-combined-random-crop/\")\n",
    "# out_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/test_results/timesformer-default-divst-v13-inclusive-with-mitosis-type-combined-no-random-crop-epoch=30\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-inclusive-all-random-crop-sequential/\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-drop-div-combined/\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-inclusive-with-mitosis-type-combined-random-crop-sampling-sequential/\")\n",
    "model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-drop-div-combined-random-crop-sequential\")\n",
    "model_name = model_dir.name\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-inclusive-video-random-crop-sequential/\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-drop-div-video\")\n",
    "# model_dir = Path(r\"./scripts/mmdetection_classify/work_dirs/timesformer-default-divst-v13-st-video\")\n",
    "out_dir = Path(\"./tmp-inference-02\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"model_dir: \", model_dir)\n",
    "print(\"model_name: \", model_name)\n",
    "print(\"out_dir: \", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a trained MMAction model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config, DictAction\n",
    "from mmaction.registry import MODELS\n",
    "# config_file = str(model_dir / \"tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py\")\n",
    "# config_file = str(model_dir / \"config_train_v12_st_cliplen=2_clipnum=3-combined.py\")\n",
    "# config_file = str(model_dir / \"config_train_v12_st_cliplen=2_clipnum=3-video.py\")\n",
    "# config_file = str(model_dir / \"config_train_timesformer_divst_v13-st-video-random-crop.py\")\n",
    "# config_file = str(model_dir / \"config_train_timesformer_divst_v13-st-combined-random-crop.py\")\n",
    "# config_file = str(model_dir / \"config_train_timesformer_divst_v13-inclusive-all-random-crop-test-random-crop.py\")\n",
    "\n",
    "# config_file = str(model_dir / \"config_train_timesformer_divst_v13-inclusive-with-mitosis-type-combined-random-crop-test-random-crop.py\")\n",
    "# checkpoint_file = str(model_dir / \"best_acc_top1_epoch_5.pth\")\n",
    "# checkpoint_file = str(model_dir / \"epoch_52.pth\")\n",
    "# checkpoint_file = str(model_dir / \"best_acc_top1_epoch_126.pth\")\n",
    "# checkpoint_file = str(model_dir / \"epoch_250.pth\")\n",
    "# checkpoint_file = str(model_dir / \"best_acc_top1_epoch_448.pth\")\n",
    "# checkpoint_file = str(model_dir / \"epoch_450.pth\")\n",
    "# checkpoint_file = str(model_dir / \"epoch_30.pth\")\n",
    "# checkpoint_file = str(model_dir / \"epoch_15.pth\")\n",
    "checkpoint_file = str(model_dir / \"epoch_30.pth\")\n",
    "# checkpoint_file = str(model_dir / \"epoch_15.pth\")\n",
    "\n",
    "# config_file = str(model_dir / \"config_train_timesformer_divst_v13-inclusive-all-random-crop-sequential.py\")\n",
    "# config_file = str(model_dir / \"config_train_timesformer_divst_v13-inclusive-with-mitosis-type-combined-random-crop-sampling-sequantial.py\")\n",
    "# config_file = str(model_dir / \"config_train_timesformer_divst_v13-inclusive-with-mitosis-type-combined-random-crop.py\")\n",
    "# config_file = str(model_dir/\"config_train_timesformer_divst_v13-inclusive-video-random-crop-sequential.py\")\n",
    "# config_file = str(model_dir/\"config_train_timesformer_divst_v13-drop-div-video.py\")\n",
    "# config_file = str(model_dir/\"config_train_timesformer_divst_v13-st-video.py\")\n",
    "\n",
    "# find the path ends with .py\n",
    "py_files_in_model_dir = list(model_dir.glob(\"*.py\"))\n",
    "if len(py_files_in_model_dir) > 1:\n",
    "    print(\"Warning: more than one .py file in model_dir, using the first one\")\n",
    "config_file = str(py_files_in_model_dir[0])\n",
    "\n",
    "\n",
    "cfg = Config.fromfile(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MODELS.build()\n",
    "import mmcv\n",
    "from mmaction.apis import init_recognizer, inference_recognizer\n",
    "\n",
    "DEVICE = 'cuda:1' # or 'cpu', based on your device\n",
    "model = init_recognizer(config_file, checkpoint_file, device=DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read single cell object data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livecellx.track.classify_utils import load_all_json_dirs\n",
    "annotation_dir = Path(\"../datasets/mitosis-annotations-2023/DIC-C2DH-HeLa/02_annotation\")\n",
    "classes = [\"mitosis\", \"mitosis_border\"]\n",
    "class2samples, class2samples_extra_info = load_all_json_dirs([annotation_dir], class_subfolders=classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set mask datasets of SCS to None because scs already contain information of contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scs = []\n",
    "for class_name, samples in all_class2samples.items():\n",
    "    for sample in samples:\n",
    "        all_scs += sample\n",
    "\n",
    "for sc in all_scs:\n",
    "    sc.mask_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scs = SingleCellStatic.load_single_cells_json(annotation_dir/\"single_cells.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scs[0].show(crop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis on Hela Cell line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from livecellx.core.single_cell import SingleCellTrajectoryCollection, SingleCellStatic\n",
    "from livecellx.track.sort_tracker_utils import (\n",
    "    track_SORT_bbox_from_scs\n",
    ")\n",
    "\n",
    "# all_scs_json_path = \"./datasets/test_scs_EBSS_starvation/tmp_corrected_scs.json\"\n",
    "# all_scs_json_path = f\"./datasets/test_scs_EBSS_starvation/{pos}/scs.json\"\n",
    "# all_scs = SingleCellStatic.load_single_cells_json(all_scs_json_path)\n",
    "sctc = track_SORT_bbox_from_scs(all_scs, raw_imgs=all_scs[0].img_dataset, min_hits=3, max_age=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_dataset = all_scs[0].img_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livecellx.core.sc_video_utils import gen_mp4_from_frames, gen_class2sample_samples, gen_samples_mp4s\n",
    "\n",
    "# importlib reload import above\n",
    "import importlib\n",
    "importlib.reload(core.sc_video_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize and analyze a specific TID sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import livecellx\n",
    "# importlib.reload(livecellx.track.classify_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cfg.test_pipeline = [\n",
    "    dict(io_backend=\"disk\", type=\"DecordInit\"),\n",
    "    dict(clip_len=8, frame_interval=1, num_clips=1, test_mode=True, type=\"SampleFrames\"),\n",
    "    dict(type=\"DecordDecode\"),\n",
    "    dict(\n",
    "        scale=(\n",
    "            -1,\n",
    "            224,\n",
    "        ),\n",
    "        type=\"Resize\",\n",
    "    ),\n",
    "    dict(crop_size=224, type=\"ThreeCrop\"),\n",
    "    dict(input_format=\"NCTHW\", type=\"FormatShape\"),\n",
    "    dict(type=\"PackActionInputs\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import livecellx\n",
    "import datetime\n",
    "importlib.reload(livecellx.track.classify_utils)\n",
    "\n",
    "pred_dir = Path(\"./notebook_results/CTC_HeLa_infer-\"+ str(model_name) + \"-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "pred_figs_dir = pred_dir / \"figs\"\n",
    "\n",
    "pred_figs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for tid, traj in sctc:\n",
    "    tid = int(tid)\n",
    "    _out_dir = pred_dir / f\"{int(tid)}\"\n",
    "    infer_result = livecellx.track.classify_utils.infer_sliding_window_traj(traj, model, \"combined\", out_dir=_out_dir, class_labels=[0], class_names=[\"mitosis\"] )\n",
    "    class_name_to_segments = infer_result[\"disjoint_segments\"]\n",
    "\n",
    "    # axes = livecellx.core.single_cell.show_sct_on_grid(traj, padding=400, start=0, interval=1, nr=9, nc=9, dims=(290, 380), dims_offset=(220, 20), ax_height=1.5, ax_width=1.9, ax_title_fontsize=13, cmap=\"binary\", ax_contour_polygon_kwargs = dict(fill=None, edgecolor='r', linewidth=0.8))\n",
    "    ax_contour_polygon_kwargs = [dict(fill=None, edgecolor='r', linewidth=0.8) for _ in range(len(traj))]\n",
    "    time_disjoint_segs = class_name_to_segments[\"mitosis\"]\n",
    "\n",
    "    for time_span in time_disjoint_segs:\n",
    "        for time in range(time_span[0], time_span[1] + 1):\n",
    "            ax_contour_polygon_kwargs[time] = dict(fill=None, edgecolor='yellow', linewidth=0.8)\n",
    "\n",
    "    axes = livecellx.core.single_cell.show_sct_on_grid(traj, padding=400, start=0, interval=1, nr=9, nc=9, dims=None, dims_offset=None, ax_height=1.5, ax_width=1.9, ax_title_fontsize=13, cmap=\"binary\", ax_contour_polygon_kwargs = ax_contour_polygon_kwargs)\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    for row in axes:\n",
    "        for ax in row:\n",
    "            ax.set_title(ax.get_title() , x=0.2, y=0.83, fontsize=11)  # adjust y as needed\n",
    "    plt.savefig(_out_dir / \"grid_traj.png\", dpi=300)\n",
    "    plt.savefig(pred_figs_dir / f\"grid_pred_{tid}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    json.dump(class_name_to_segments, open(_out_dir / \"class_name_to_segments.json\", \"w\"))\n",
    "    print(class_name_to_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize all the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tid, track in sctc:\n",
    "#     print(\"TID:\", tid)\n",
    "#     track.show_on_grid(padding=100, start=0, interval=1)\n",
    "#     plt.title(\"TID:\" + str(tid))\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livecellx.track.classify_utils import gen_tid2samples_by_window, gen_inference_sctc_sample_videos\n",
    "sample_tid = 3\n",
    "# sample_tid = 26\n",
    "sample_traj = sctc.get_trajectory(sample_tid)\n",
    "tmp_sctc = SingleCellTrajectoryCollection()\n",
    "tmp_sctc.add_trajectory(sample_traj)\n",
    "# tid2samples, tid2start_end_times = livecellx.track.classify_utils.gen_tid2samples_by_window(tmp_sctc, window_size=8)\n",
    "# sample_tid_samples = tid2samples[sample_tid]\n",
    "specific_sample_output_dir = Path(f\"./tmp_hela_specific_test_samples_data\")\n",
    "specific_traj_video_df = gen_inference_sctc_sample_videos(tmp_sctc, padding_pixels=[200], out_dir=specific_sample_output_dir, prefix=f\"specific-sample-test\", fps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_traj_video_df[\"frame_type\"].unique(), specific_traj_video_df[\"track_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_traj), len(sample_traj[0].img_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import livecellx\n",
    "# importlib.reload(livecellx.core.single_cell)\n",
    "sample_traj = sctc.get_trajectory(10)\n",
    "axes = livecellx.core.single_cell.show_sct_on_grid(sample_traj, padding=400, start=0, interval=1, nr=9, nc=9, dims=(290, 380), dims_offset=(0, 20), ax_height=1.5, ax_width=1.9, ax_title_fontsize=13, cmap=\"binary\", ax_contour_polygon_kwargs = dict(fill=None, edgecolor='r', linewidth=0.8))\n",
    "plt.subplots_adjust(hspace=0)\n",
    "for row in axes:\n",
    "    for ax in row:\n",
    "        ax.set_title(ax.get_title() , x=0.2, y=0.83, fontsize=11)  # adjust y as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_types = specific_traj_video_df[\"frame_type\"].unique()\n",
    "for i, frame_type in enumerate(frame_types):\n",
    "    # report stats of frame type\n",
    "    frame_type_df = specific_traj_video_df[specific_traj_video_df[\"frame_type\"] == frame_type]\n",
    "    # report number of samples\n",
    "    print(f\"Frame type: {frame_type}, number of samples: {len(frame_type_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "mitosis_tids = set()\n",
    "mitosis_save_dir = specific_sample_output_dir / f\"mitosis\"\n",
    "mitosis_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "specific_traj_combined_df = specific_traj_video_df[specific_traj_video_df[\"frame_type\"] == \"combined\"]\n",
    "specific_traj_mask_df = specific_traj_video_df[specific_traj_video_df[\"frame_type\"] == \"mask\"]\n",
    "specific_traj_raw_df = specific_traj_video_df[specific_traj_video_df[\"frame_type\"] == \"video\"]\n",
    "\n",
    "# selected_df = specific_traj_raw_df\n",
    "# selected_df = specific_traj_combined_df\n",
    "selected_df = specific_traj_video_df\n",
    "\n",
    "def insert_time_segments(new_segment: Tuple[int, int], disjoint_segments: list):\n",
    "    \"\"\"add the new segment to segments, merge if there is overlap between new_segment and any segment in segments, keep all segments non-overlapping\"\"\"\n",
    "    if len(disjoint_segments) == 0:\n",
    "        disjoint_segments.append(new_segment)\n",
    "        return\n",
    "    # find the first segment that overlaps with new_segment\n",
    "    merged = False\n",
    "    for i, segment in enumerate(disjoint_segments):\n",
    "        if segment[0] <= new_segment[0] <= segment[1] or segment[0] <= new_segment[1] <= segment[1]:\n",
    "            # merge the new segment with the segment\n",
    "            disjoint_segments[i] = (min(segment[0], new_segment[0]), max(segment[1], new_segment[1]))\n",
    "            merged = True\n",
    "    # no overlap found, add the new segment\n",
    "    if not merged:\n",
    "        disjoint_segments.append(new_segment)\n",
    "        disjoint_segments.sort(key=lambda x: x[0])\n",
    "        return\n",
    "    # check if there is any overlap between segments\n",
    "    for i in range(len(disjoint_segments) - 1):\n",
    "        if disjoint_segments[i][1] >= disjoint_segments[i+1][0]:\n",
    "            # merge the two segments\n",
    "            disjoint_segments[i] = (min(disjoint_segments[i][0], disjoint_segments[i+1][0]), max(disjoint_segments[i][1], disjoint_segments[i+1][1]))\n",
    "            # remove the second segment\n",
    "            disjoint_segments.pop(i+1)\n",
    "\n",
    "    return disjoint_segments\n",
    "\n",
    "disjoint_segments = []\n",
    "for i, row in tqdm(selected_df.iterrows(), total = len(selected_df)):\n",
    "    video_filename = row[\"path\"]\n",
    "    video_path = str(specific_sample_output_dir / \"videos\" / video_filename)\n",
    "\n",
    "    # Inference\n",
    "    results = inference_recognizer(model, video_path)\n",
    "    if \"pred_label\" in results.keys():\n",
    "        # TimeSformer\n",
    "        predicted_label = results.pred_label\n",
    "    else:\n",
    "        # TSN\n",
    "        predicted_label = results.pred_labels.item.cpu().numpy()[0]\n",
    "        \n",
    "    if predicted_label == 0:\n",
    "        print(\"mitosis found:\", row[\"path\"])\n",
    "        # print start, end time\n",
    "        start_time = row[\"start_time\"]\n",
    "        end_time = row[\"end_time\"]\n",
    "        insert_time_segments((start_time, end_time), disjoint_segments)\n",
    "        print(start_time, end_time)\n",
    "        print(\"frame type:\", row[\"frame_type\"])\n",
    "        # copy the video file to mitosis folder\n",
    "        import shutil\n",
    "        shutil.copy(video_path, str(specific_sample_output_dir / f\"mitosis/{video_filename}\"))\n",
    "\n",
    "mitosis_tids, disjoint_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livecellx.track.classify_utils import gen_tid2samples_by_window, gen_inference_sctc_sample_videos\n",
    "sc_samples = []\n",
    "samples_info_list = []\n",
    "sample_output_dir = Path(f\"./tmp_hela_test_samples_{pos}_data\")\n",
    "\n",
    "tid2samples, tid2start_end_times = gen_tid2samples_by_window(sctc, window_size=7)\n",
    "for tid, samples in tid2samples.items():\n",
    "    for i, sample in enumerate(samples):\n",
    "        sc_samples.append(sample)\n",
    "        samples_info_list.append({\"tid\": tid, \"sample_idx\": i})\n",
    "\n",
    "saved_sample_info_df = gen_inference_sctc_sample_videos(sctc, padding_pixels=[20], out_dir=sample_output_dir, prefix=f\"{pos}\", fps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_sample_info_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitosis_tids = set()\n",
    "for i, row in tqdm(saved_sample_info_df.iterrows()):\n",
    "    video_path = row[\"path\"]\n",
    "    video_path = str(sample_output_dir / \"videos\" / video_path)\n",
    "    results = inference_recognizer(model, video_path)\n",
    "    if \"pred_label\" in results.keys():\n",
    "        # TimeSformer\n",
    "        predicted_label = results.pred_label\n",
    "    else:\n",
    "        # TSN\n",
    "        predicted_label = results.pred_labels.item.cpu().numpy()[0]\n",
    "    if predicted_label == 0:\n",
    "        print(\"mitosis found:\", row[\"path\"])\n",
    "        mitosis_tids.add(row[\"track_id\"])\n",
    "mitosis_tids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.video.io import VideoReader\n",
    "import tempfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livecellx.track.classify_utils import video_frames_and_masks_from_sample, combine_video_frames_and_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sctc.get_all_trajectories()[11].get_all_scs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import uuid\n",
    "# video = r\"D:\\LiveCellTracker-dev\\notebooks\\notebook_results\\mmaction_train_data\\videos\\mitosis_1_mask.mp4\"\n",
    "# video = r\"D:\\LiveCellTracker-dev\\notebooks\\notebook_results\\mmaction_train_data\\videos\\normal_6.mp4\"\n",
    "# input_frames = sct_input_frames[:5]\n",
    "\n",
    "def predict_on_frames_windows(model, frames, window_size=5, step_size=1, fps = 1):\n",
    "    frame_windows = []\n",
    "    for i in range(0, len(frames), step_size):\n",
    "        if i + window_size > len(frames):\n",
    "            break\n",
    "        frame_window = frames[i:i+window_size]\n",
    "        frame_windows.append(frame_window)\n",
    "    results = []\n",
    "    for frame_window in frame_windows:\n",
    "        results.append(predict_on_frames(model, frame_window, fps))\n",
    "    return results\n",
    "\n",
    "def predict_on_frames(model, input_frames, fps, tmp_dir = \"./tmp_video\", tmp_filename=None):\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    if tmp_filename is None:\n",
    "        tmp_id = uuid.uuid4()\n",
    "        tmp_filename = os.path.join(tmp_dir, f\"tmp-{tmp_id}.mp4\")\n",
    "\n",
    "    def save_frames(frames, filename):\n",
    "        with open(filename, \"wb+\") as tmp_video_file:\n",
    "            # mmcv.frames2video(frames, tmp_video_file.name, fps=1)\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            # Create a VideoWriter object to write the frames to the output file\n",
    "            height, width, channels = frames[0].shape\n",
    "            video_writer = cv2.VideoWriter(tmp_video_file.name, fourcc, fps, (width, height))\n",
    "\n",
    "            # Write the frames to the output file\n",
    "            for frame in frames:\n",
    "                video_writer.write(frame)\n",
    "            video_writer.release()\n",
    "            tmp_video_file.flush()\n",
    "            return tmp_video_file\n",
    "        \n",
    "    saved_tmp_video_file = save_frames(input_frames, tmp_filename)\n",
    "    results = inference_recognizer(model, saved_tmp_video_file.name)\n",
    "    predicted_label = results.pred_labels.item.cpu().numpy()[0]\n",
    "    new_filename = os.path.join(tmp_dir, f\"label-{predicted_label}-tmp-{tmp_id}.mp4\")\n",
    "\n",
    "    # convert the first channel in input_frames to RGB and save the video\n",
    "    input_frames = np.array(input_frames)\n",
    "    input_frames_rgb = [cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB) for frame in input_frames[..., 0]]\n",
    "    raw_frame_video_filename = os.path.join(tmp_dir, f\"label-{predicted_label}-tmp-{tmp_id}-raw.mp4\")\n",
    "    save_frames(input_frames_rgb, raw_frame_video_filename)\n",
    "\n",
    "    # move file to new filename\n",
    "    os.rename(tmp_filename, new_filename)\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = predict_on_frame_windows(model, sct_input_frames, window_size=20, step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on sctc samples\n",
    "from tqdm import tqdm\n",
    "def predict_on_sctc_samples(model, sctc, window_size=5, step_size=1, fps = 3, padding_pixels=0):\n",
    "    def _gen_sc_frames_combined(sample, padding_pixels=0):\n",
    "        frames, frame_masks = video_frames_and_masks_from_sample(sample, padding_pixels=padding_pixels)\n",
    "        combined_frames = combine_video_frames_and_masks(frames, frame_masks)\n",
    "        sct_input_frames = [np.array(frame) for frame in combined_frames]\n",
    "        return sct_input_frames\n",
    "\n",
    "    all_samples_results = {}\n",
    "    for track_id, trajectory in tqdm(sctc):\n",
    "        sample = trajectory.get_all_scs()\n",
    "        sct_input_frames = _gen_sc_frames_combined(sample, padding_pixels=padding_pixels)\n",
    "        sample_results = predict_on_frames_windows(model, sct_input_frames, window_size=window_size, step_size=step_size, fps=fps)\n",
    "        if 0 in sample_results:\n",
    "            print(\"track_id:\", track_id, \"contains mitosis\")\n",
    "        all_samples_results[track_id] = sample_results\n",
    "    return all_samples_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sctc.write_json(f\"./datasets/test_scs_EBSS_starvation/{pos}/sctc.json\", dataset_json_dir=f\"./datasets/test_scs_EBSS_starvation/{pos}/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./tmp_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track_id_to_sample_results = predict_on_sctc_samples(model, sctc, window_size=10, step_size=5, fps=1, padding_pixels=20)\n",
    "track_id_to_sample_results = predict_on_sctc_samples(model, sctc, window_size=7, step_size=1, fps=3, padding_pixels=40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize mitosis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livecellx.core.sct_operator import create_scts_operator_viewer\n",
    "\n",
    "# mitosis_tids = [7, 4, 1, 16] # EBSS less than 24h, xy1, by <tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb-v6-combined-clipL=2>\n",
    "mitosis_tids = [tid for tid in track_id_to_sample_results if 0 in track_id_to_sample_results[tid]]\n",
    "mitosis_tids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livecellx.core.sct_operator import create_scts_operator_viewer, create_scs_edit_viewer\n",
    "\n",
    "# scts_operator = create_scs_edit_viewer(single_cells, img_dataset = dic_dataset, time_span=(145, 155))\n",
    "# # If you would like to start from sctc, you can use the following code\n",
    "# scts_operator = create_scts_operator_viewer(sctc, img_dataset = dic_dataset) #, time_span=(1, 2))\n",
    "# scts_operator = create_scts_operator_viewer(sctc.subset(mitosis_tids), img_dataset = dic_dataset) #, time_span=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = inference_recognizer(model, r\"D:\\LiveCellTracker-dev\\notebooks\\notebook_results\\mmaction_train_data_v5\\videos\\mitosis_2_combined_padding-40.mp4\")\n",
    "# results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test dataframe and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# video_dir = r\"./notebook_results/mmaction_train_data_v5/videos\"\n",
    "# mmaction_data_tsv = r\"./notebook_results/mmaction_train_data_v6/test_data_combined.txt\"\n",
    "# video_dir = r\"./notebook_results/mmaction_train_data_v8/videos\"\n",
    "# mmaction_data_tsv = r\"./notebook_results/mmaction_train_data_v8/mmaction_test_data_combined.txt\"\n",
    "# mmaction_data_tsv = r\"./notebook_results/mmaction_train_data_v8/mmaction_train_data_combined.txt\"\n",
    "# video_dir = r\"./notebook_results/mmaction_train_data_v9/videos\"\n",
    "# mmaction_data_tsv = r\"./notebook_results/mmaction_train_data_v9/mmaction_test_data_combined.txt\"\n",
    "# video_dir = r\"./notebook_results/mmaction_train_data_v10-st/videos\"\n",
    "# mmaction_data_tsv = r\"./notebook_results/mmaction_train_data_v10-st/mmaction_test_data_combined.txt\"\n",
    "# video_dir = r\"./notebook_results/mmaction_train_data_v10-drop-div/videos\"\n",
    "# mmaction_data_tsv = r\"./notebook_results/mmaction_train_data_v10-drop-div/mmaction_test_data_combined.txt\"\n",
    "\n",
    "video_dir = r\"./notebook_results/mmaction_train_data_v12-st/videos\"\n",
    "mmaction_data_tsv = r\"./notebook_results/mmaction_train_data_v12-st/mmaction_test_data_combined.txt\"\n",
    "\n",
    "\n",
    "# df columns are path and label\n",
    "mmaction_data_df = pd.read_csv(mmaction_data_tsv, sep=\" \", header=None)\n",
    "mmaction_data_df.columns = [\"path\", \"label\"]\n",
    "mmaction_data_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# test on all video in the df\n",
    "video_paths = []\n",
    "gt_labels = []\n",
    "for i, row in tqdm(mmaction_data_df.iterrows()):\n",
    "    video_path = os.path.join(video_dir, row[\"path\"])\n",
    "    video_paths.append(video_path)\n",
    "    gt_labels.append(row[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt2total = {}\n",
    "gt2correct = {}\n",
    "for i, video_path in tqdm(enumerate(video_paths)):\n",
    "    results = inference_recognizer(model, video_path)\n",
    "    predicted_label = results.pred_labels.item.cpu().numpy()[0]\n",
    "    gt_label = gt_labels[i]\n",
    "    if gt_label not in gt2total:\n",
    "        gt2total[gt_label] = 0\n",
    "        gt2correct[gt_label] = 0\n",
    "    gt2total[gt_label] += 1\n",
    "    if predicted_label != gt_labels[i]:\n",
    "        print(\"wrong prediction:\", video_path, \"predicted_label:\", predicted_label, \"gt_label:\", gt_labels[i])\n",
    "    else:\n",
    "        gt2correct[gt_label] += 1\n",
    "\n",
    "for gt_label, total in gt2total.items():\n",
    "    correct = gt2correct[gt_label]\n",
    "    print(\"gt_label:\", gt_label, \"total:\", total, \"correct:\", correct, \"acc:\", correct / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "livecell-work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
