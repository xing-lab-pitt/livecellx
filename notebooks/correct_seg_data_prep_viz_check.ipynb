{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbca04ff5b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "from livecellx.model_zoo.segmentation.sc_correction import CorrectSegNet\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tqdm\n",
    "torch.manual_seed(237)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available(), torch.cuda.current_device(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "from livecellx.model_zoo.segmentation.sc_correction import CorrectSegNet\n",
    "from livecellx.model_zoo.segmentation.sc_correction_dataset import CorrectSegNetDataset\n",
    "torch.manual_seed(237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>seg</th>\n",
       "      <th>gt</th>\n",
       "      <th>raw_seg</th>\n",
       "      <th>scale</th>\n",
       "      <th>aug_diff_mask</th>\n",
       "      <th>gt_label_mask</th>\n",
       "      <th>raw_transformed_img</th>\n",
       "      <th>subdir</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>real_overseg_td1_XY9_dropout</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data_v11/r...</td>\n",
       "      <td>real_overseg_td1_XY9_dropout</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "1  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "\n",
       "                                                 seg  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "1  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "\n",
       "                                                  gt  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "1  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "\n",
       "                                             raw_seg  scale  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data_v11/r...    0.0   \n",
       "1  notebook_results/a549_ccp_vim/train_data_v11/r...    0.0   \n",
       "\n",
       "                                       aug_diff_mask  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "1  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "\n",
       "                                       gt_label_mask  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "1  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "\n",
       "                                 raw_transformed_img  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "1  notebook_results/a549_ccp_vim/train_data_v11/r...   \n",
       "\n",
       "                         subdir  Unnamed: 0  \n",
       "0  real_overseg_td1_XY9_dropout         NaN  \n",
       "1  real_overseg_td1_XY9_dropout         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# train_dir = Path(\"./notebook_results/a549_ccp_vim/train_data_v4/\")\n",
    "# test_dir = Path(\"./notebook_results/a549_ccp_vim/test_data_v4/\")\n",
    "# train_dir = Path(\"./notebook_results/a549_ccp_vim/train_data_v5/\")\n",
    "# test_dir = Path(\"./notebook_results/a549_ccp_vim/test_data_v5/\")\n",
    "\n",
    "# train_dir = Path(\"./notebook_results/a549_ccp_vim/train_data_v8/\")\n",
    "# test_dir = Path(\"./notebook_results/a549_ccp_vim/test_data_v8/\")\n",
    "# train_csv = train_dir / \"train_data.csv\"\n",
    "\n",
    "# train_csv = \"./notebook_results/a549_ccp_vim/train_real_td_data/real_underseg_td1_XY8/data.csv\"\n",
    "# train_csv = \"./notebook_results/a549_ccp_vim/train_real_td_data/real_overseg_td1_XY9/data.csv\"\n",
    "# train_csv = \"./notebook_results/a549_ccp_vim/train_data_v11/test_overseg_dropout/data.csv\"\n",
    "# train_csv = \"/home/ken67/LiveCellTracker-dev/notebooks/notebook_results/a549_ccp_vim/test_data_v11/real_overseg_td1_XY5_dropout/data.csv\"\n",
    "train_csv = \"/home/ken67/LiveCellTracker-dev/notebooks/notebook_results/a549_ccp_vim/train_data_v11/train_data.csv\"\n",
    "# train_csv = \"/home/ken67/LiveCellTracker-dev/notebooks/notebook_results/a549_ccp_vim/test_data_v11/train_data.csv\"\n",
    "# train_csv = \"D:\\\\LiveCellTracker-dev\\\\datasets\\\\yaxuan_csn_annotation\\\\train_data.csv\"\n",
    "# train_csv = \"/home/ken67/LiveCellTracker-dev/notebooks/notebook_results/a549_ccp_vim/train_data_v6/synthetic_overseg/data.csv\"\n",
    "# train_csv = \"/home/ken67/LiveCellTracker-dev/notebooks/notebook_results/a549_ccp_vim/train_data_v9/synthetic_overseg/data.csv\"\n",
    "train_df = pd.read_csv(train_csv)\n",
    "\n",
    "if \"subdir\" not in train_df.columns:\n",
    "    train_df[\"subdir\"] = \"cur\"\n",
    "# test_df = pd.read_csv(test_dir / \"train_data.csv\")\n",
    "train_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all gt labels: any empty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livecellx.core.datasets import read_img_default\n",
    "# check all gt label's unique labels\n",
    "gt_paths = train_df[\"gt\"].values\n",
    "\n",
    "for path in gt_paths:\n",
    "    # read tiff\n",
    "    gt_tif = read_img_default(path)\n",
    "    # print(\"gt shape:\", gt_tif.shape)\n",
    "    # print(\"unique labels:\", np.unique(gt_tif))\n",
    "    assert np.unique(gt_tif.flatten()).shape[0] > 1, \"#unique labels should be > 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken67/anaconda3/envs/livecell/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ken67/anaconda3/envs/livecell/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Using MSE loss\n",
      ">>> Based on loss type, training output threshold:  1\n",
      "input type: raw_aug_duplicate\n",
      "if apply_gt_seg_edt: True\n",
      "whether to normalize_uint8: False\n",
      "input type: raw_aug_duplicate\n",
      "if apply_gt_seg_edt: True\n",
      "whether to normalize_uint8: False\n"
     ]
    }
   ],
   "source": [
    "from livecellx.model_zoo.segmentation.eval_csn import compute_metrics, assemble_train_test_dataset\n",
    "\n",
    "model = CorrectSegNet.load_from_checkpoint(r\"/home/ken67/LiveCellTracker-dev/notebooks/lightning_logs/version_v11_02/checkpoints/epoch=3819-global_step=0.ckpt\")\n",
    "train_dataset_eval, val_dataset_eval, test_dataset_eval, whole_dataset_eval = assemble_train_test_dataset(train_df, train_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input type: raw_aug_duplicate\n",
      "if apply_gt_seg_edt: True\n",
      "whether to normalize_uint8: False\n"
     ]
    }
   ],
   "source": [
    "from livecellx.model_zoo.segmentation.eval_csn import assemble_dataset, assemble_train_test_dataset\n",
    "\n",
    "# split_seed = 237\n",
    "# dataset = assemble_dataset(train_df, apply_gt_seg_edt = model.apply_gt_seg_edt, exclude_raw_input_bg=model.exclude_raw_input_bg, input_type=model.input_type)\n",
    "# train_sample_num = int(len(dataset) * 0.8)\n",
    "# val_sample_num = len(dataset) - train_sample_num\n",
    "# split_generator = torch.Generator().manual_seed(split_seed)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "#     dataset, [train_sample_num, val_sample_num], generator=split_generator\n",
    "# )\n",
    "\n",
    "# test_dataset = assemble_dataset(test_df, apply_gt_seg_edt = model.apply_gt_seg_edt, exclude_raw_input_bg=model.exclude_raw_input_bg, input_type=model.input_type)\n",
    "train_dataset = assemble_dataset(train_df, apply_gt_seg_edt = True, exclude_raw_input_bg=False, input_type=\"raw_aug_duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_dataset.transform =     transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((412, 412))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viz training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.normalize_uint8=False\n",
    "train_dataset.apply_gt_seg_edt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 1455)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(train_dataset.raw_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_dataset)):\n",
    "    sample = train_dataset[idx]\n",
    "    gt_mask = sample[\"gt_mask\"]\n",
    "    assert np.unique(gt_tif.flatten()).shape[0] > 1, \"#unique labels should be > 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 131, 103])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"input\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1455 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken67/anaconda3/envs/livecell/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1455 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample input shape: torch.Size([3, 412, 412])\n",
      "model input shape: torch.Size([1, 3, 412, 412])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      7\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> 8\u001b[0m metrics \u001b[39m=\u001b[39m livecellx\u001b[39m.\u001b[39;49mmodel_zoo\u001b[39m.\u001b[39;49msegmentation\u001b[39m.\u001b[39;49meval_csn\u001b[39m.\u001b[39;49mcompute_metrics(train_dataset, model, whole_dataset\u001b[39m=\u001b[39;49mtrain_dataset)\n",
      "File \u001b[0;32m~/LiveCellTracker-dev/livecellx/model_zoo/segmentation/eval_csn.py:230\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(dataset, model, out_threshold, whole_dataset, gt_label_masks)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     gt_label_mask \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_gt_label_mask(i)\n\u001b[0;32m--> 230\u001b[0m single_sample_metrics \u001b[39m=\u001b[39m evaluate_sample_v3_underseg(\n\u001b[1;32m    231\u001b[0m     sample, model, out_threshold\u001b[39m=\u001b[39;49mout_threshold, gt_label_mask\u001b[39m=\u001b[39;49mgt_label_mask\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    233\u001b[0m \u001b[39mfor\u001b[39;00m metric, value \u001b[39min\u001b[39;00m single_sample_metrics\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m metric \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m res_metrics:\n",
      "File \u001b[0;32m~/LiveCellTracker-dev/livecellx/model_zoo/segmentation/eval_csn.py:166\u001b[0m, in \u001b[0;36mevaluate_sample_v3_underseg\u001b[0;34m(sample, model, raw_seg, scale, out_threshold, gt_label_mask, gt_iou_match_thresholds)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m# match gt label mask with out label mask\u001b[39;00m\n\u001b[1;32m    165\u001b[0m out_label_mask \u001b[39m=\u001b[39m skimage\u001b[39m.\u001b[39mmeasure\u001b[39m.\u001b[39mlabel(out_mask_predicted)\n\u001b[0;32m--> 166\u001b[0m out_matched_num, out_cell_count, gt_cell_num, gt_out_iou_list \u001b[39m=\u001b[39m match_label_mask_by_iou(\n\u001b[1;32m    167\u001b[0m     out_label_mask, gt_label_mask, match_threshold\u001b[39m=\u001b[39;49mgt_iou_match_thresholds[\u001b[39m0\u001b[39;49m], return_iou_list\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    168\u001b[0m )\n\u001b[1;32m    169\u001b[0m origin_matched_num, origin_cell_count, gt_cell_num, gt_origin_iou_list \u001b[39m=\u001b[39m match_label_mask_by_iou(\n\u001b[1;32m    170\u001b[0m     original_label_mask, gt_label_mask, return_iou_list\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    173\u001b[0m metrics_dict \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/LiveCellTracker-dev/livecellx/model_zoo/segmentation/eval_csn.py:81\u001b[0m, in \u001b[0;36mmatch_label_mask_by_iou\u001b[0;34m(out_label_mask, gt_label_mask, bg_label, match_threshold, return_iou_list)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatch_label_mask_by_iou\u001b[39m(out_label_mask, gt_label_mask, bg_label\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, match_threshold\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, return_iou_list\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39massert\u001b[39;00m out_label_mask\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m gt_label_mask\u001b[39m.\u001b[39mshape\n\u001b[1;32m     83\u001b[0m     out_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(out_label_mask)\n\u001b[1;32m     84\u001b[0m     gt_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(gt_label_mask)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from livecellx.model_zoo.segmentation.eval_csn import compute_metrics\n",
    "import importlib\n",
    "import livecellx.model_zoo.segmentation.eval_csn\n",
    "importlib.reload(livecellx.model_zoo.segmentation.eval_csn)\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "metrics = livecellx.model_zoo.segmentation.eval_csn.compute_metrics(train_dataset, model, whole_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def viz_sample_only(sample):\n",
    "    fig, axes = plt.subplots(1, 7, figsize=(3 * 7, 3))\n",
    "    ax_idx = 0\n",
    "    ax = axes[ax_idx]\n",
    "    ax.imshow(sample[\"input\"][0])\n",
    "    ax.set_title(\"input: dim0\")\n",
    "\n",
    "    ax_idx += 1\n",
    "    ax = axes[ax_idx]\n",
    "    ax.imshow(sample[\"input\"][1])\n",
    "    ax.set_title(\"input: dim1\")\n",
    "\n",
    "    ax_idx += 1\n",
    "    ax = axes[ax_idx]\n",
    "    ax.imshow(sample[\"input\"][2])\n",
    "    ax.set_title(\"input:dim2\")\n",
    "\n",
    "    # gt\n",
    "    ax_idx += 1\n",
    "    ax = axes[ax_idx]\n",
    "    ax.imshow(sample[\"gt_mask\"][0])\n",
    "    ax.set_title(\"gt0 seg\")\n",
    "\n",
    "    ax_idx += 1\n",
    "    ax = axes[ax_idx]\n",
    "    ax.imshow(sample[\"gt_mask\"][1])\n",
    "    ax.set_title(\"gt1 seg\")\n",
    "\n",
    "    ax_idx += 1\n",
    "    ax = axes[ax_idx]\n",
    "    ax.imshow(sample[\"gt_mask\"][2])\n",
    "    ax.set_title(\"gt2 seg\")\n",
    "\n",
    "    ax_idx += 1\n",
    "    ax = axes[ax_idx]\n",
    "    ax.imshow(sample[\"gt_label_mask\"])\n",
    "    ax.set_title(\"gt label mask\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "out_dir = Path(\"./tmp/csn_check_data_v11/\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "for i in range(50):\n",
    "    sample_id = random.randint(0, len(train_dataset) - 1)\n",
    "    sample = train_dataset[sample_id]\n",
    "    df_row = train_dataset.raw_df.iloc[sample_id]\n",
    "    print(\"source subdir:\", df_row[\"subdir\"])\n",
    "    print(\"sample id:\", sample_id)\n",
    "    # plt.hist(sample[\"input\"][0].flatten(), bins=100)\n",
    "    # plt.hist(sample[\"gt_mask\"][1].flatten(), bins=100)\n",
    "    viz_sample_only(sample)\n",
    "    plt.suptitle(f\"sample id: {sample_id}, raw_df subdir: {df_row['subdir']}, raw_path: {df_row['raw']}\")\n",
    "    # plt.subplots_adjust(top=0.55)\n",
    "    plt.tight_layout()\n",
    "    plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "    plt.savefig(out_dir / f\"sample_{sample_id}.png\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in tqdm.tqdm(train_dataset):\n",
    "    gt_label_mask = sample[\"gt_label_mask\"]\n",
    "    assert np.unique(gt_label_mask).shape[0] > 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "livecell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c7226d793827cd27273ad20fbb4775c3cb91053ab9378a09de5f8c6f258919"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
