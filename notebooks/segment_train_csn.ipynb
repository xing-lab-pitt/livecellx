{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a correct segment network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ke/anaconda3/envs/livecell/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f302c13e170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>seg</th>\n",
       "      <th>gt</th>\n",
       "      <th>raw_seg</th>\n",
       "      <th>scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data/raw/i...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data/augme...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data/gt/im...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data/seg/i...</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data/raw/i...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data/augme...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data/gt/im...</td>\n",
       "      <td>notebook_results/a549_ccp_vim/train_data/seg/i...</td>\n",
       "      <td>-0.233333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data/raw/i...   \n",
       "1  notebook_results/a549_ccp_vim/train_data/raw/i...   \n",
       "\n",
       "                                                 seg  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data/augme...   \n",
       "1  notebook_results/a549_ccp_vim/train_data/augme...   \n",
       "\n",
       "                                                  gt  \\\n",
       "0  notebook_results/a549_ccp_vim/train_data/gt/im...   \n",
       "1  notebook_results/a549_ccp_vim/train_data/gt/im...   \n",
       "\n",
       "                                             raw_seg     scale  \n",
       "0  notebook_results/a549_ccp_vim/train_data/seg/i... -0.300000  \n",
       "1  notebook_results/a549_ccp_vim/train_data/seg/i... -0.233333  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "train_dir = Path(\"./notebook_results/a549_ccp_vim/train_data/\")\n",
    "train_csv = train_dir / \"train_data.csv\"\n",
    "train_df = pd.read_csv(train_csv)\n",
    "train_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img_paths = list(train_df[\"raw\"])\n",
    "seg_mask_paths = list(train_df[\"seg\"])\n",
    "gt_mask_paths = list(train_df[\"gt\"])\n",
    "raw_seg_paths = list(train_df[\"raw_seg\"])\n",
    "scales = list(train_df[\"scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ke/anaconda3/envs/livecell/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ke/anaconda3/envs/livecell/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from livecell_tracker.model_zoo.segmentation.sc_correction_dataset import CorrectSegNetDataset\n",
    "dataset = CorrectSegNetDataset(raw_img_paths, seg_mask_paths, gt_mask_paths, raw_seg_paths=raw_seg_paths, scales=scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('notebook_results/a549_ccp_vim/train_data/raw/img-1_seg-14.tif',\n",
       "  'notebook_results/a549_ccp_vim/train_data/augmented_seg/img-1_seg-14_aug-0.tif',\n",
       "  'notebook_results/a549_ccp_vim/train_data/gt/img-1_seg-14.tif',\n",
       "  'notebook_results/a549_ccp_vim/train_data/seg/img-1_seg-14.tif',\n",
       "  -0.3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_tuples = list(zip(raw_img_paths, seg_mask_paths, gt_mask_paths, raw_seg_paths, scales))\n",
    "train_input_tuples[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_results/a549_ccp_vim/train_data/raw/img-1_seg-14.tif\n",
      "notebook_results/a549_ccp_vim/train_data/augmented_seg/img-1_seg-14_aug-0.tif\n",
      "notebook_results/a549_ccp_vim/train_data/gt/img-1_seg-14.tif\n",
      "notebook_results/a549_ccp_vim/train_data/seg/img-1_seg-14.tif\n",
      "-0.3\n"
     ]
    }
   ],
   "source": [
    "for data in list(zip(*train_input_tuples)):\n",
    "    print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch transform example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 412, 412]),\n",
       " tensor(15162.4404),\n",
       " torch.Size([3, 441, 427]),\n",
       " tensor(15174.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize((412, 412)),\n",
    "        transforms.RandomCrop((412, 412)),\n",
    "        transforms.RandomHorizontalFlip(412, 412, padd_if_needed=True),\n",
    "        transforms.RandomAffine(degrees=360, translate=(1, 1), scale=(0.5, 1.5)),\n",
    "    ]\n",
    ")\n",
    "train_transforms(dataset[0][\"input\"]).shape, train_transforms(dataset[0][\"input\"])[0, 0, 0], dataset[0][\"input\"].shape,dataset[0][\"input\"][0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information.\n",
      "\n",
      "\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information.\n",
      "\n",
      "\n",
      "/home/ke/anaconda3/envs/livecell/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ke/anaconda3/envs/livecell/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/ke/anaconda3/envs/livecell/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | model         | DeepLabV3        | 42.0 M\n",
      "1 | loss_func     | CrossEntropyLoss | 0     \n",
      "2 | val_accuracy  | Accuracy         | 0     \n",
      "3 | test_accuracy | Accuracy         | 0     \n",
      "---------------------------------------------------\n",
      "42.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "42.0 M    Total params\n",
      "167.997   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ke/anaconda3/envs/livecell/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ke/anaconda3/envs/livecell/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64:   7%|▋         | 20/280 [00:09<02:01,  2.15it/s, loss=0.0021, v_num=33, val_loss=0.00201, val_acc=0.999]  "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "from livecell_tracker.model_zoo.segmentation.sc_correction import CorrectSegNet\n",
    "\n",
    "# TODO: decide augment in dataset object or in model?\n",
    "img_paths, mask_paths, gt_paths, seg_paths, scales= list(zip(*train_input_tuples))\n",
    "dataset = CorrectSegNetDataset(img_paths, mask_paths, gt_paths, raw_seg_paths=seg_paths, scales=scales, transform=train_transforms)\n",
    "\n",
    "train_sample_num = int(len(dataset) * 0.8)\n",
    "val_sample_num = len(dataset) - train_sample_num\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_sample_num, val_sample_num])\n",
    "\n",
    "model = CorrectSegNet(train_input_paths=train_input_tuples, num_workers=1, batch_size=2, train_transforms=train_transforms, train_dataset=train_dataset, val_dataset=val_dataset, test_dataset=val_dataset)\n",
    "trainer = Trainer(gpus=1, max_epochs=500)\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('livecell')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44711c8e940e07c59d9fa231abc5a6460e2a468757eff674995cd9414270dce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
